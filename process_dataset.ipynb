{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb15d70",
   "metadata": {},
   "source": [
    "### Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4093fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import dlib\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c714c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ear(eye_points):\n",
    "    A = distance.euclidean(eye_points[1], eye_points[5])\n",
    "    B = distance.euclidean(eye_points[2], eye_points[4])\n",
    "    C = distance.euclidean(eye_points[0], eye_points[3])\n",
    "    ear = (A+B) / (2.0 * C)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12740aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mar(mouth_points):\n",
    "    A = distance.euclidean(mouth_points[2], mouth_points[8])\n",
    "    B =  distance.euclidean(mouth_points[3], mouth_points[7])\n",
    "    C = distance.euclidean(mouth_points[4], mouth_points[6])\n",
    "    D = distance.euclidean(mouth_points[0], mouth_points[5])\n",
    "    mar = (A + B + C) / (2.0 * D)\n",
    "    return mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8833ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_head_angle(landmarks):\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0), # Nariz\n",
    "        (0.0, -330.0, -65.0), # Barbilla\n",
    "        (-225.0, 170.0, -135.0), # Ojo izquierdo\n",
    "        (225.0, 170.0, -135.0), # Ojo derecho\n",
    "        (-150.0, -150.0, -125.0), # Boca izquierda\n",
    "        (150.0, -150.0, -125.0) # Boca derecha\n",
    "    ])\n",
    "    \n",
    "    image_points = np.array([\n",
    "        (landmarks.part(30).x, landmarks.part(30).y), # Nariz\n",
    "        (landmarks.part(8).x, landmarks.part(8).y), # Barbilla\n",
    "        (landmarks.part(36).x, landmarks.part(36).y), # Ojo izquierdo\n",
    "        (landmarks.part(45).x, landmarks.part(45).y), # Ojo derecho\n",
    "        (landmarks.part(48).x, landmarks.part(48).y), # Boca izquierda\n",
    "        (landmarks.part(54).x, landmarks.part(54).y) # Boca derecha\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "    focal_length = 640\n",
    "    center = (640/2, 480/2)\n",
    "    camera_matrix = np.array([[focal_length, 0, center[0]],\n",
    "                              [0, focal_length, center[1]],\n",
    "                              [0, 0, 1]], dtype=\"double\")\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "\n",
    "    # Usa la función cv2.solvePnP (Perspective-n-Point) de OpenCV para estimar la rotación y traslación de la cabeza en 3D.\n",
    "    # 'rotation_vector': Un vector de rotación 3D (en radianes) que describe la orientación de la cabeza (ángulos de Euler en los ejes X, Y, Z).\n",
    "    _, rotation_vector, _ = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs)\n",
    "    # np.linalg.norm(rotation_vector): Calcula la magnitud del vector de rotación, que representa la rotación total de la cabeza (en radianes).\n",
    "    # * 180 / np.pi: Convierte la magnitud de radianes a grados.\n",
    "    angle = np.linalg.norm(rotation_vector) * 180 / np.pi\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fb1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_blinks(ears, ear_threshold=0.2):\n",
    "    blinks = 0\n",
    "    total_closed_frames = 0\n",
    "    current_closed_frames = 0\n",
    "    max_closed_duration = 0\n",
    "    prev_ear = ears[0]\n",
    "\n",
    "    for ear in ears[1:]:\n",
    "        if ear <= ear_threshold:\n",
    "            current_closed_frames += 1\n",
    "            total_closed_frames += 1\n",
    "        else:\n",
    "            max_closed_duration = max(max_closed_duration, current_closed_frames)\n",
    "            current_closed_frames = 0\n",
    "        \n",
    "        if prev_ear > ear_threshold and ear <= ear_threshold:\n",
    "            blinks += 1\n",
    "            \n",
    "        prev_ear = ear\n",
    "\n",
    "    if current_closed_frames > 0:\n",
    "        max_closed_duration = max(max_closed_duration, current_closed_frames)\n",
    "    \n",
    "    return {\n",
    "        'blinks': blinks,\n",
    "        'total_closed_frames': total_closed_frames,\n",
    "        'max_closed_duration': max_closed_duration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6305066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_image(image_path, detector, predictor):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    faces = detector(img)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(img, face)\n",
    "        left_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)]\n",
    "        right_eye = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)]\n",
    "        mouth = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(48, 68)]\n",
    "        ear = (calculate_ear(left_eye) + calculate_ear(right_eye)) / 2.0\n",
    "        mar = calculate_mar(mouth)\n",
    "        head_angle = calculate_head_angle(landmarks)\n",
    "        return {'ear': ear, 'mar': mar, 'head_angle': head_angle}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319bfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(drowsy_dir, notdrowsy_dir, detector, predictor, window_size=20, meta_window_size=5):\n",
    "    videos = {}\n",
    "\n",
    "    # Procesar carpeta drowsy\n",
    "    for img_name in os.listdir(drowsy_dir):\n",
    "        match = re.match(r\"(\\d+_\\w+_\\w+)_(\\d+)_drowsy\", img_name)\n",
    "        if match:\n",
    "            video_id_base, frame_num = match.groups()\n",
    "            video_id = f\"{video_id_base}_drowsy\"\n",
    "            if video_id not in videos:\n",
    "                videos[video_id] = {'frames': [], 'label': 1}\n",
    "            videos[video_id]['frames'].append((int(frame_num), os.path.join(drowsy_dir, img_name)))\n",
    "    \n",
    "    # Procesar carpeta notdrowsy\n",
    "    for img_name in os.listdir(notdrowsy_dir):\n",
    "        match = re.match(r\"(\\d+_\\w+_\\w+)_(\\d+)_notdrowsy\", img_name)\n",
    "        if match:\n",
    "            video_id_base, frame_num = match.groups()\n",
    "            video_id = f\"{video_id_base}_notdrowsy\"\n",
    "            if video_id not in videos:\n",
    "                videos[video_id] = {'frames': [], 'label': 0}\n",
    "            videos[video_id]['frames'].append((int(frame_num), os.path.join(notdrowsy_dir, img_name)))\n",
    "\n",
    "    # Ordenar frames y extraer caracteristicas\n",
    "    data = []\n",
    "    for video_id, info in videos.items():\n",
    "        info['frames'].sort() # Ordenar por número de fotograma\n",
    "        print(f\"Video {video_id}: Inicio = {info['frames'][0][0]}, Fin = {info['frames'][-1][0]}, Etiqueta = {info['label']}, Imagenes = {len(info['frames'])}\")\n",
    "\n",
    "        ears, mars, head_angles = [], [], []\n",
    "        for _, frame_path in info['frames']:\n",
    "            features = extract_features_from_image(frame_path, detector, predictor)\n",
    "            if features:\n",
    "                ears.append(features['ear'])\n",
    "                mars.append(features['mar'])\n",
    "                head_angles.append(features['head_angle'])\n",
    "        \n",
    "        if len(ears) >= window_size:\n",
    "            window_metrics = []\n",
    "            for i in range(0, len(ears) - window_size + 1):\n",
    "                window_ears = ears[i:i+window_size]\n",
    "                window_mars = mars[i:i+window_size]\n",
    "                window_angles = head_angles[i:i+window_size]\n",
    "                blink_info = count_blinks(window_ears)\n",
    "                window_metrics.append({\n",
    "                    'blink_freq': blink_info['blinks'],\n",
    "                    'total_closed_frames': blink_info['total_closed_frames'],\n",
    "                    'max_closed_duration': blink_info['max_closed_duration'],\n",
    "                    'ear_mean': np.mean(window_ears),\n",
    "                    'ear_std': np.std(window_ears),\n",
    "                    'ear_min': np.min(window_ears),\n",
    "                    'mar_mean': np.mean(window_mars),\n",
    "                    'mar_std': np.std(window_mars),\n",
    "                    'mar_max': np.max(window_mars),\n",
    "                    'head_angle_mean': np.mean(window_angles)\n",
    "                })\n",
    "            \n",
    "            for j in range(0, len(window_metrics) - meta_window_size + 1):\n",
    "                meta_window = window_metrics[j:j+meta_window_size]\n",
    "                total_closed_in_meta = sum(w['total_closed_frames'] for w in meta_window)\n",
    "                percent_closed_in_meta = (total_closed_in_meta / (window_size*meta_window_size)) * 100\n",
    "                mostly_closed_windows = sum(1 for w in meta_window if(w['total_closed_frames'] / window_size) > 0.5)\n",
    "                data.append({\n",
    "                    'video_id': video_id,\n",
    "                    'ear_mean': meta_window[-1]['ear_mean'],\n",
    "                    'ear_std': meta_window[-1]['ear_std'],\n",
    "                    'ear_min': meta_window[-1]['ear_min'],\n",
    "                    'mar_mean': meta_window[-1]['mar_mean'],\n",
    "                    'mar_std': meta_window[-1]['mar_std'],\n",
    "                    'mar_max': meta_window[-1]['mar_max'],\n",
    "                    'head_angle_mean': meta_window[-1]['head_angle_mean'],\n",
    "                    'blink_freq': meta_window[-1]['blink_freq'],\n",
    "                    'total_closed_frames': meta_window[-1]['total_closed_frames'],\n",
    "                    'max_closed_duration': meta_window[-1]['max_closed_duration'],\n",
    "                    'percent_closed_in_meta': percent_closed_in_meta,\n",
    "                    'mostly_closed_windows': mostly_closed_windows,\n",
    "                    'label': info['label']\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e01591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 001_glasses_sleepyCombination_drowsy: Inicio = 599, Fin = 2747, Etiqueta = 1, Imagenes = 2149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface_landmarks/shape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Descarga este archivo\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Procesar dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrowsy_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotdrowsy_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaracterísticas guardadas en features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(drowsy_dir, notdrowsy_dir, detector, predictor, window_size, meta_window_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m ears, mars, head_angles \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, frame_path \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 32\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features:\n\u001b[0;32m     34\u001b[0m         ears\u001b[38;5;241m.\u001b[39mappend(features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mear\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mextract_features_from_image\u001b[1;34m(image_path, detector, predictor)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m      7\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m predictor(img, face)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Directorios del dataset\n",
    "drowsy_dir = \"dataset/drowsy\"\n",
    "notdrowsy_dir = \"dataset/notdrowsy\"\n",
    "\n",
    "# Cargar detector y predictor\n",
    "\n",
    "# Esta línea crea un detector de rostros frontales usando la biblioteca DLib. El método get_frontal_face_detector() utiliza un clasificador \n",
    "# preentrenado basado en Histogram of Oriented Gradients (HOG) para detectar rostros en una imagen. Devuelve un objeto que puede identificar \n",
    "# regiones en una imagen donde hay rostros.\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Crea un predictor de puntos faciales (landmarks) usando DLib. El archivo \"shape_predictor_68_face_landmarks.dat\" es un modelo preentrenado \n",
    "# que identifica 68 puntos específicos en un rostro (como ojos, nariz, boca, contornos). Este predictor toma una región de rostro detectada \n",
    "# por el 'detector' y devuelve las coordenadas de esos 68 puntos.\n",
    "predictor = dlib.shape_predictor(\"face_landmarks/shape_predictor_68_face_landmarks.dat\")  # Descarga este archivo\n",
    "\n",
    "# Procesar dataset\n",
    "df = process_dataset(drowsy_dir, notdrowsy_dir, detector, predictor)\n",
    "df.to_csv(\"features.csv\", index=False)\n",
    "print(\"Características guardadas en features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
